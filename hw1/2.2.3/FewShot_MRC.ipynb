{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5f1kcwr6V6g"
      },
      "source": [
        "# **Question Answering‚ùì**\n",
        "with fine-tuned BERT on SQuAD 2.0.  \n",
        "\n",
        "Question answering comes in many forms. We‚Äôll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. More specifically, we will fine tune the [bert-base-uncased](https://huggingface.co/bert-base-uncased) model on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.\n",
        "\n",
        "I have followed [this tutorial](https://huggingface.co/transformers/v3.2.0/custom_datasets.html#question-answering-with-squad-2-0) from the huggingface community for how to fine tune BERT on custom datasets which in our case is the SQuAD 2.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a082DBXQ62fG"
      },
      "source": [
        "**Some first imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KQI4beK724N",
        "outputId": "2ecdd456-25db-4290-daed-51f4ab907002"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqBFb_WjGdFI"
      },
      "source": [
        "**Connecting Google Drive in order to save the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ressj5OzXrCc"
      },
      "outputs": [],
      "source": [
        "# if not os.path.exists('/content/drive/MyDrive/BERT-SQuAD'):\n",
        "#   os.mkdir('/content/drive/MyDrive/BERT-SQuAD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE9Wlhn0Dmww",
        "outputId": "88abdb13-2bed-4e12-b719-9553dec1e46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/bowen/.local/lib/python3.10/site-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (0.13.0)\n",
            "Requirement already satisfied: filelock in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (1.23.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/bowen/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bowen/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mQrxq_g8GKv"
      },
      "source": [
        "### **Download SQuAD 2.0 ‚¨áÔ∏è**\n",
        "\n",
        "SQuAD consists of two json files.\n",
        "\n",
        "* train dataset \n",
        "* validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRY07swS9DPH",
        "outputId": "4d83cbf1-6870-4f4f-ac9f-f71688ca2619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-14 18:32:43--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‚Äòtrain-v2.0.json‚Äô\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   253MB/s    in 0.2s    \n",
            "\n",
            "2023-03-14 18:32:43 (253 MB/s) - ‚Äòtrain-v2.0.json‚Äô saved [42123633/42123633]\n",
            "\n",
            "--2023-03-14 18:32:43--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‚Äòdev-v2.0.json‚Äô\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-03-14 18:32:44 (127 MB/s) - ‚Äòdev-v2.0.json‚Äô saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6EgciGr1xSg"
      },
      "source": [
        "## **Data preprocessing üíΩ**\n",
        "\n",
        "In short, we have to do the following:\n",
        "\n",
        "1. Extract the data from the jsons files\n",
        "2. Tokenize the data\n",
        "3. Define the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WE2gdsy7SMk"
      },
      "source": [
        "### **Get data üìÅ** \n",
        "\n",
        "After we got a taste of the jsons files data format let's extract our data and store them into some data structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F5wGx6bJ-BJC"
      },
      "outputs": [],
      "source": [
        "def read_data(path):  \n",
        "  # load the json file\n",
        "  with open(path, 'rb') as f:\n",
        "    squad = json.load(f)\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "\n",
        "  for group in squad['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "      context = passage['context']\n",
        "      for qa in passage['qas']:\n",
        "        question = qa['question']\n",
        "        for answer in qa['answers']:\n",
        "          contexts.append(context)\n",
        "          questions.append(question)\n",
        "          answers.append(answer)\n",
        "\n",
        "  return contexts, questions, answers\n",
        "\n",
        "random.seed(42)\n",
        "def split_dataset(contexts, questions, answers):\n",
        "  num_elements = int(len(contexts) * 0.1)\n",
        "  sample_indices = random.sample(range(len(contexts)), num_elements)\n",
        "\n",
        "  contexts_sample = [contexts[i] for i in sample_indices]\n",
        "  questions_sample = [questions[i] for i in sample_indices]\n",
        "  answers_sample = [answers[i] for i in sample_indices]\n",
        "  \n",
        "  return contexts_sample, questions_sample, answers_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeKCd5W4SEKY"
      },
      "source": [
        "Put the contexts, questions and answers for training and validation into the appropriate lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An5wY1MNEwrt",
        "outputId": "afa75b2d-9873-438a-ec74-85d5215d54ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8682 20302\n"
          ]
        }
      ],
      "source": [
        "train_contextl, train_questionl, train_answerl = read_data('squad/train-v2.0.json')\n",
        "train_contexts, train_questions, train_answers = split_dataset(train_contextl, train_questionl, train_answerl)\n",
        "valid_contexts, valid_questions, valid_answers = read_data('squad/dev-v2.0.json')\n",
        "print(len(train_contexts), len(valid_contexts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2AHo87fDYHy"
      },
      "source": [
        "As you can see above, the answers are dictionaries whith the answer text and an integer which indicates the start index of the answer in the context. As the SQuAD does not give us the end index of the answer in the context we have to find it ourselves. So, let's get the character position at which the answer ends in the passage. Note that sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9kiFd7LF6xa",
        "outputId": "a89b21c5-f864-415a-9093-bf842f751718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The often unclear division between a self-executing treaty and a non-self-executing treaty can lead to a treaty being what if disagreements exist within a party?\n",
            "{'text': 'politicized', 'answer_start': 61, 'answer_end': 72}\n"
          ]
        }
      ],
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "  for answer, context in zip(answers, contexts):\n",
        "    gold_text = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "\n",
        "    # sometimes squad answers are off by a character or two so we fix this\n",
        "    if context[start_idx:end_idx] == gold_text:\n",
        "      answer['answer_end'] = end_idx\n",
        "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "      answer['answer_start'] = start_idx - 1\n",
        "      answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "      answer['answer_start'] = start_idx - 2\n",
        "      answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(valid_answers, valid_contexts)\n",
        "# You can see that now we get the answer_end also\n",
        "print(train_questions[-1000])\n",
        "print(train_answers[-1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsVHwdOaHWis"
      },
      "source": [
        "### **Tokenization üî¢**\n",
        "\n",
        "As we know we have to tokenize our data in form that is acceptable for the BERT model. We are going to use the `BertTokenizerFast` instead of `BertTokenizer` as the first one is much faster. Since we are going to train our model in batches we need to set `padding=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM1xi8PjuJ8c",
        "outputId": "4165d95b-1c75-49cc-f049-71c2edceef85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "model_name = \"bert-base-cased\"\n",
        "# Load a pre-trained checkpoint\n",
        "checkpoint_path = 'squad/checkpoint-57950'\n",
        "\n",
        "# load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# # Load the checkpoint into the model\n",
        "# pre_trained_weight = torch.load(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
        "\n",
        "# # pre_trained_weight.pop('bert.pooler.dense.weight')\n",
        "# # pre_trained_weight.pop('bert.pooler.dense.bias')\n",
        "# weight = pre_trained_weight.pop('classifier.weight')\n",
        "# bias = pre_trained_weight.pop('classifier.bias')\n",
        "\n",
        "# pre_trained_weight['qa_outputs.weight'] = weight[:2]\n",
        "# pre_trained_weight['qa_outputs.bias'] = bias[:2]\n",
        "\n",
        "# model.load_state_dict(pre_trained_weight)\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gADJPJH_VBcQ"
      },
      "source": [
        "Next we need to convert our character start/end positions to token start/end positions. Why is that? Because our words converted into tokens, so the answer start/end needs to show the index of start/end token which contains the answer and not the specific characters in the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "r7uXDM0mMfOo"
      },
      "outputs": [],
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "    # if start position is None, the answer passage has been truncated\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(valid_encodings, valid_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRnqbmjD3Lip"
      },
      "source": [
        "### **Dataset definition üóÑÔ∏è**\n",
        "\n",
        "We have to define our dataset using the PyTorch Dataset class from `torch.utils` in order create our dataloaders after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JC5t8iOyORg-"
      },
      "outputs": [],
      "source": [
        "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SQuAD_Dataset(train_encodings)\n",
        "valid_dataset = SQuAD_Dataset(valid_encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8APUSaCswlID"
      },
      "source": [
        "### **Dataloaders üîÅ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ROBQQLmNwGbb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Define the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkkgktrwPYhy"
      },
      "source": [
        "## **Fine-Tuning ‚öôÔ∏è**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybwjqNH72qAS"
      },
      "source": [
        "### **Model definition ü§ñ**\n",
        "\n",
        "We are going to use the `bert-case-uncased` from the huggingface transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKY-irY25_F"
      },
      "source": [
        "### **Training üèãÔ∏è‚Äç‚ôÇÔ∏è**\n",
        "\n",
        "Œúy choices for some parameters:\n",
        "\n",
        "* Use of `AdamW` which is a stochastic optimization method that modifies the typical implementation of weight decay in Adam, by decoupling weight decay from the gradient update. This helps to avoid overfitting which is necessary in this case were the model is very complex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh_2--xXwW6o",
        "outputId": "bc0f216d-3bb6-4b48-bb09-f832541631fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on cuda\n"
          ]
        }
      ],
      "source": [
        "# Check on the available device - use GPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Working on {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOXHj73UEh7f",
        "outputId": "0db5f065-bc69-45dc-d8b3-aea30ad20ff5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bowen/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  0%|          | 0/136 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 7.79 GiB total capacity; 6.81 GiB already allocated; 57.62 MiB free; 6.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/home/bowen/Code/BERT/CMSC828aHW1_Q223_v1.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bowen/Code/BERT/CMSC828aHW1_Q223_v1.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m start_positions \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mstart_positions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bowen/Code/BERT/CMSC828aHW1_Q223_v1.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m end_positions \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mend_positions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bowen/Code/BERT/CMSC828aHW1_Q223_v1.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, start_positions\u001b[39m=\u001b[39;49mstart_positions, end_positions\u001b[39m=\u001b[39;49mend_positions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bowen/Code/BERT/CMSC828aHW1_Q223_v1.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bowen/Code/BERT/CMSC828aHW1_Q223_v1.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1851\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1839\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m \u001b[39mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m \u001b[39m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[39m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1851\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1852\u001b[0m     input_ids,\n\u001b[1;32m   1853\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1854\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1855\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1856\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1857\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1858\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1859\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1860\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1861\u001b[0m )\n\u001b[1;32m   1863\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1865\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqa_outputs(sequence_output)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1019\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1010\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1012\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1013\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1014\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1018\u001b[0m )\n\u001b[0;32m-> 1019\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1020\u001b[0m     embedding_output,\n\u001b[1;32m   1021\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1022\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1023\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1024\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1026\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1027\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1029\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1030\u001b[0m )\n\u001b[1;32m   1031\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1032\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:609\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    600\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    601\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    602\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    610\u001b[0m         hidden_states,\n\u001b[1;32m    611\u001b[0m         attention_mask,\n\u001b[1;32m    612\u001b[0m         layer_head_mask,\n\u001b[1;32m    613\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    614\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    615\u001b[0m         past_key_value,\n\u001b[1;32m    616\u001b[0m         output_attentions,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    619\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:357\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    359\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 7.79 GiB total capacity; 6.81 GiB already allocated; 57.62 MiB free; 6.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Freeze the backbone parameters\n",
        "# for param in model.bert.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "train_losses = []\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  loss_of_epoch = 0\n",
        "\n",
        "  for batch in loop:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    loss_of_epoch += loss.item()\n",
        "    loop.set_description(f'Epoch {epoch+1}')\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "  loss_of_epoch /= len(train_loader)\n",
        "  train_losses.append(loss_of_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPYSDEgi2FcM",
        "outputId": "a39b17d8-d218-4930-8710-b2b177869ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8.546129494905472, 6.665331488146501, 5.701449467855341, 5.44834376783932, 5.330090836567037]\n"
          ]
        }
      ],
      "source": [
        "print(train_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFY9n5hSFjfT"
      },
      "source": [
        "**Save the model in my drive in order not to run it each time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4NXG5UGWWQ9",
        "outputId": "f3d6d88f-001a-4416-cf42-3b4df8d03c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/BERT-SQuAD/MRC_finetuen/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/BERT-SQuAD/MRC_finetuen/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/BERT-SQuAD/MRC_finetuen/vocab.txt',\n",
              " '/content/drive/MyDrive/BERT-SQuAD/MRC_finetuen/added_tokens.json',\n",
              " '/content/drive/MyDrive/BERT-SQuAD/MRC_finetuen/tokenizer.json')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# model_path = '/content/drive/MyDrive/BERT-SQuAD/Model_weights_HW1/NER/MRC_11590'\n",
        "# model.save_pretrained(model_path)\n",
        "# tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuFwUaF8BR3H"
      },
      "source": [
        "**Respectively, load the saved model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aslUE82N73mu"
      },
      "outputs": [],
      "source": [
        "#from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
        "\n",
        "#model_path = '/content/drive/MyDrive/BERT-SQuAD'\n",
        "#model = BertForQuestionAnswering.from_pretrained(model_path)\n",
        "#tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#print(f'Working on {device}')\n",
        "\n",
        "#model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIuUtJhBFo5l"
      },
      "source": [
        "### **Testing ‚úÖ**\n",
        "\n",
        "We are evaluating the model on the validation set by checking the model's predictions for the answer's start and end indexes and comparing with the true ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB37KYRZJlEF"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "\n",
        "# acc = []\n",
        "\n",
        "\n",
        "# for batch in tqdm(valid_loader):\n",
        "\n",
        "#   with torch.no_grad():\n",
        "#     input_ids = batch['input_ids'].to(device)\n",
        "#     attention_mask = batch['attention_mask'].to(device)\n",
        "#     start_true = batch['start_positions'].to(device)\n",
        "#     end_true = batch['end_positions'].to(device)\n",
        "    \n",
        "#     outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "#     start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "#     end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "\n",
        "#     acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "#     acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfpsVIUB5dH-"
      },
      "outputs": [],
      "source": [
        "# print(acc)\n",
        "# accuracy = sum(acc)/len(acc)\n",
        "# print(\"Validation Accuracy is {}\".format(accuracy))\n",
        "\n",
        "# print(\"\\n\\nT/P\\tanswer_start\\tanswer_end\\n\")\n",
        "# for i in range(len(start_true)):\n",
        "#   print(f\"true\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
        "#         f\"pred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIU46_teig2E",
        "outputId": "c8852ab8-cab1-41fc-cd8c-689da37c99b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20302/20302 [05:10<00:00, 65.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy is 0.06334351295438873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "valid_predictions = []\n",
        "val_answers = []\n",
        "acc = []\n",
        "for i in tqdm(range(len(valid_contexts))):\n",
        "    encoded_dict = tokenizer.encode_plus(valid_questions[i], valid_contexts[i], return_tensors='pt', max_length=512, truncation=True)\n",
        "    input_ids = encoded_dict['input_ids'].to(device)\n",
        "    token_type_ids = encoded_dict['token_type_ids'].to(device)\n",
        "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
        "    start_true = batch['start_positions'].to(device)\n",
        "    end_true = batch['end_positions'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_logits, dim=1)[0]\n",
        "    end_index = torch.argmax(end_logits, dim=1)[0]\n",
        "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "    acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "    acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "    valid_predictions.append(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0][start_index:end_index+1])))\n",
        "    val_answers.append(valid_answers[i]['text'])\n",
        "\n",
        "accuracy = sum(acc)/len(acc)\n",
        "print(\"Validation Accuracy is {}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o-pG0Q33HeIp"
      },
      "outputs": [],
      "source": [
        "def get_prediction(context, question):\n",
        "  inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
        "  outputs = model(**inputs)\n",
        "  \n",
        "  answer_start = torch.argmax(outputs[0])  \n",
        "  answer_end = torch.argmax(outputs[1]) + 1 \n",
        "  \n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "  \n",
        "  return answer\n",
        "\n",
        "def normalize_text(s):\n",
        "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "  import string, re\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "  def white_space_fix(text):\n",
        "    return \" \".join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def exact_match(prediction, truth):\n",
        "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "  pred_tokens = normalize_text(prediction).split()\n",
        "  truth_tokens = normalize_text(truth).split()\n",
        "  \n",
        "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "  \n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "  \n",
        "  # if there are no common tokens then f1 = 0\n",
        "  if len(common_tokens) == 0:\n",
        "    return 0\n",
        "  \n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "  F1 = round(2 * (prec * rec) / (prec + rec), 2)\n",
        "  # print(prec, rec, F1)\n",
        "  return F1\n",
        "  \n",
        "def question_answer(context, question, answer):\n",
        "  prediction = get_prediction(context,question)\n",
        "  em_score = exact_match(prediction, answer)\n",
        "  f1_score = compute_f1(prediction, answer)\n",
        "\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Prediction: {prediction}')\n",
        "  print(f'True Answer: {answer}')\n",
        "  print(f'Exact match: {em_score}')\n",
        "  print(f'F1 score: {f1_score}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA__tP-UBIOr",
        "outputId": "8711e2fc-5536-4400-9b5a-85819a9bdb6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validate number: 20302\n",
            "Exact match number: 279\n",
            "Exact match: 0.013742488424785735\n",
            "F1 score:    0.04945473352378949\n"
          ]
        }
      ],
      "source": [
        "EM = []\n",
        "F1 = []\n",
        "Prec = []\n",
        "Rec = []\n",
        "for pre, val in zip(valid_predictions, val_answers):\n",
        "  em_score = exact_match(pre, val)\n",
        "  f1_score = compute_f1(pre, val)\n",
        "  EM.append(em_score)\n",
        "  F1.append(f1_score)\n",
        "  # Prec.append(prec)\n",
        "  # Rec.append(rec)\n",
        "\n",
        "print(f'Validate number: {len(EM)}')\n",
        "print(f'Exact match number: {sum(EM)}')\n",
        "print(f'Exact match: {sum(EM)/len(EM)}')\n",
        "# print(f'Precision:   {sum(EM)/len(EM)}')\n",
        "# print(f'Recall:      {sum(EM)/len(EM)}')\n",
        "print(f'F1 score:    {sum(F1)/len(F1)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoNiMv_HF2AD"
      },
      "source": [
        "### **Ask questions üôã**\n",
        "\n",
        "We are going to use some functions from the [*official Evaluation Script v2.0*](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/) of SQuAD in order to test the fine-tuned model by asking some questions given a context. I have also looked at this [notebook](https://colab.research.google.com/github/fastforwardlabs/ff14_blog/blob/master/_notebooks/2020-06-09-Evaluating_BERT_on_SQuAD.ipynb#scrollTo=MzPlHgWEBQ8D) which evaluates BERT on SQuAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afOyVOyoLE7Y"
      },
      "source": [
        "**Beyonc√©**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2bitFpuS0WY",
        "outputId": "36ee8566-2054-4710-f420-923c92f24710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: For whom the passage is talking about?\n",
            "Prediction: \n",
            "True Answer: Beyonce Giselle Knowles - Carter\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: When did Beyonce born?\n",
            "Prediction: \n",
            "True Answer: September 4, 1981\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: Where did Beyonce born?\n",
            "Prediction: \n",
            "True Answer: Houston, Texas\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: What is Beyonce's nationality?\n",
            "Prediction: \n",
            "True Answer: American\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: Who was the Destiny's group manager?\n",
            "Prediction: \n",
            "True Answer: Mathew Knowles\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: What name has the Beyonc√©'s debut album?\n",
            "Prediction: \n",
            "True Answer: Dangerously in Love\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: How many Grammy Awards did Beyonce earn?\n",
            "Prediction: \n",
            "True Answer: five\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: When did the Beyonc√©'s debut album release?\n",
            "Prediction: \n",
            "True Answer: 2003\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: Who was the lead singer of R&B girl-group Destiny's Child?\n",
            "Prediction: \n",
            "True Answer: Beyonce Giselle Knowles - Carter\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, \n",
        "          songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing \n",
        "          and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. \n",
        "          Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. \n",
        "          Their hiatus saw the release of Beyonc√©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, \n",
        "          earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\"\"\"\n",
        "\n",
        "\n",
        "questions = [\"For whom the passage is talking about?\",\n",
        "             \"When did Beyonce born?\",\n",
        "             \"Where did Beyonce born?\",\n",
        "             \"What is Beyonce's nationality?\",\n",
        "             \"Who was the Destiny's group manager?\",\n",
        "             \"What name has the Beyonc√©'s debut album?\",\n",
        "             \"How many Grammy Awards did Beyonce earn?\",\n",
        "             \"When did the Beyonc√©'s debut album release?\",\n",
        "             \"Who was the lead singer of R&B girl-group Destiny's Child?\"]\n",
        "\n",
        "answers = [\"Beyonce Giselle Knowles - Carter\", \"September 4, 1981\", \"Houston, Texas\", \n",
        "           \"American\", \"Mathew Knowles\", \"Dangerously in Love\", \"five\", \"2003\", \n",
        "           \"Beyonce Giselle Knowles - Carter\"]\n",
        "\n",
        "for question, answer in zip(questions, answers):\n",
        "  question_answer(context, question, answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clBBH3gWLMjI"
      },
      "source": [
        "**Athens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBos0I6qL1ht",
        "outputId": "e4d42b47-e492-4d10-d657-d82e2f4530d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which is the largest city in Greece?\n",
            "Prediction: Plato ' s Academy\n",
            "True Answer: Athens\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: For what was the Athens center?\n",
            "Prediction: Classical Athens was a powerful city - state. It was a center for the arts, learning and philosophy, and the home of Plato ' s Academy\n",
            "True Answer: center for the arts, learning and philosophy\n",
            "Exact match: False\n",
            "F1 score: 0.46\n",
            "\n",
            "Question: Which city was the home of Plato's Academy?\n",
            "Prediction: Plato ' s Academy\n",
            "True Answer: Athens\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"Athens is the capital and largest city of Greece. Athens dominates the Attica region and is one of the world's oldest cities, \n",
        "             with its recorded history spanning over 3,400 years and its earliest human presence starting somewhere between the 11th and 7th millennium BC.\n",
        "             Classical Athens was a powerful city-state. It was a center for the arts, learning and philosophy, and the home of Plato's Academy and Aristotle's Lyceum.\n",
        "             It is widely referred to as the cradle of Western civilization and the birthplace of democracy, largely because of its cultural and political impact on the European continent‚Äîparticularly Ancient Rome.\n",
        "             In modern times, Athens is a large cosmopolitan metropolis and central to economic, financial, industrial, maritime, political and cultural life in Greece. \n",
        "             In 2021, Athens' urban area hosted more than three and a half million people, which is around 35% of the entire population of Greece.\n",
        "             Athens is a Beta global city according to the Globalization and World Cities Research Network, and is one of the biggest economic centers in Southeastern Europe. \n",
        "             It also has a large financial sector, and its port Piraeus is both the largest passenger port in Europe, and the second largest in the world.\"\"\"\n",
        "\n",
        "questions = [\"Which is the largest city in Greece?\",\n",
        "             \"For what was the Athens center?\",\n",
        "             \"Which city was the home of Plato's Academy?\"]\n",
        "\n",
        "answers = [\"Athens\", \"center for the arts, learning and philosophy\", \"Athens\"]\n",
        "\n",
        "for question, answer in zip(questions, answers):\n",
        "  question_answer(context, question, answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6K24hLNad7c"
      },
      "source": [
        "**Angelos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEGIRvK2afqn",
        "outputId": "f92849da-e22e-44cc-fbc1-3056a5172f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: When did Angelos born?\n",
            "Prediction: Angelos born? [SEP] Angelos Pouli\n",
            "True Answer: 8 April 2001\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: In what university is Angelos studying now?\n",
            "Prediction: Angelos studying now? [SEP] Angelos Pouli\n",
            "True Answer: University of Athens\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: What is Angelos' nationality?\n",
            "Prediction: ##s Pouli\n",
            "True Answer: half Cypriot and half Greek\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: What are his scientific interests?\n",
            "Prediction: ##s Pouli\n",
            "True Answer: Artificial Intelligence\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n",
            "Question: What I will do right now?\n",
            "Prediction: ##s Pouli\n",
            "True Answer: stop talking about me\n",
            "Exact match: False\n",
            "F1 score: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"Angelos Poulis was born on 8 April 2001 in Nicosia, Cyprus. He is half Cypriot and half Greek. \n",
        "            He is currently studying at the Department of Informatics and Telecommunications of the University of Athens in Greece. \n",
        "            His scientific interests are in the broad field of Artificial Intelligence and he loves to train neural networks! \n",
        "            Okay, I'm Angelos and I'll stop talking about me right now.\"\"\"\n",
        "\n",
        "questions = [\"When did Angelos born?\",\n",
        "             \"In what university is Angelos studying now?\",\n",
        "             \"What is Angelos' nationality?\",\n",
        "             \"What are his scientific interests?\",\n",
        "             \"What I will do right now?\"]\n",
        "\n",
        "answers = [\"8 April 2001\", \"University of Athens\", \n",
        "           \"half Cypriot and half Greek\", \"Artificial Intelligence\", \n",
        "           \"stop talking about me\"]\n",
        "\n",
        "for question, answer in zip(questions, answers):\n",
        "  question_answer(context, question, answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
